{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5f9eb3",
   "metadata": {},
   "source": [
    "https://lucene.apache.org/core/8_7_0/core/overview-summary.html#overview.description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0547d1f",
   "metadata": {},
   "source": [
    "https://lucene.apache.org/core/8_7_0/core/org/apache/lucene/analysis/package-summary.html#package.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8878f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@file:DependsOn(\"org.apache.lucene:lucene-core:8.7.0\")\n",
    "@file:DependsOn(\"org.apache.lucene:lucene-queryparser:8.7.0\")\n",
    "@file:DependsOn(\"org.apache.lucene:lucene-analyzers-common:8.7.0\")\n",
    "@file:DependsOn(\"com.jianggujin:IKAnalyzer-lucene:8.0.0\")\n",
    "import java.io.*\n",
    "import java.nio.file.*\n",
    "import org.apache.lucene.analysis.standard.*\n",
    "import org.apache.lucene.analysis.core.*\n",
    "import org.apache.lucene.store.*\n",
    "import org.apache.lucene.index.*\n",
    "import org.apache.lucene.document.*\n",
    "import org.apache.lucene.search.*\n",
    "import org.apache.lucene.util.*\n",
    "import org.apache.lucene.analysis.tokenattributes.*\n",
    "import org.apache.lucene.queryparser.classic.*\n",
    "import org.wltea.analyzer.lucene.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a853d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fieldname:text\r\n",
      "1\r\n",
      "This is the text to be indexed.\r\n"
     ]
    }
   ],
   "source": [
    "val analyzer = StandardAnalyzer();\n",
    "\n",
    "val path = Path.of(\"data/lucene/temp\")\n",
    "val indexPath = Files.createDirectory(path)\n",
    "val directory = FSDirectory.open(indexPath)\n",
    "val config = IndexWriterConfig(analyzer);\n",
    "val iwriter = IndexWriter(directory, config);\n",
    "val doc = Document();\n",
    "val text = \"This is the text to be indexed.\";\n",
    "doc.add(Field(\"fieldname\", text, TextField.TYPE_STORED));\n",
    "iwriter.addDocument(doc);\n",
    "iwriter.close();\n",
    "\n",
    "// Now search the index:\n",
    "val ireader = DirectoryReader.open(directory);\n",
    "val isearcher = IndexSearcher(ireader);\n",
    "// Parse a simple query that searches for \"text\":\n",
    "val parser = QueryParser(\"fieldname\", analyzer);\n",
    "val query = parser.parse(\"text\");\n",
    "println(query.toString())\n",
    "val hits = isearcher.search(query, 10).scoreDocs;\n",
    "println(hits.size);\n",
    "// Iterate through the results:\n",
    "for (i in 0..hits.size-1) {\n",
    "  val hitDoc = isearcher.doc(hits[i].doc);\n",
    "  println(hitDoc.get(\"fieldname\"))\n",
    "}\n",
    "ireader.close();\n",
    "directory.close();\n",
    "IOUtils.rm(indexPath);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73ffddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: term=今,bytes=[e4 bb 8a],startOffset=0,endOffset=1,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n",
      "token: term=天,bytes=[e5 a4 a9],startOffset=1,endOffset=2,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n",
      "token: term=是,bytes=[e6 98 af],startOffset=2,endOffset=3,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n",
      "token: term=高,bytes=[e9 ab 98],startOffset=3,endOffset=4,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n",
      "token: term=兴,bytes=[e5 85 b4],startOffset=4,endOffset=5,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n",
      "token: term=的,bytes=[e7 9a 84],startOffset=5,endOffset=6,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n",
      "token: term=一,bytes=[e4 b8 80],startOffset=6,endOffset=7,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n",
      "token: term=天,bytes=[e5 a4 a9],startOffset=7,endOffset=8,positionIncrement=1,positionLength=1,type=<IDEOGRAPHIC>,termFrequency=1\n"
     ]
    }
   ],
   "source": [
    "val analyzer = StandardAnalyzer();\n",
    "val ts = analyzer.tokenStream(\"myfield\", StringReader(\"今天是高兴的一天\"));\n",
    "\n",
    " try {\n",
    "   ts.reset();\n",
    "   while (ts.incrementToken()) {\n",
    "     println(\"token: \" + ts.reflectAsString(false));\n",
    "   }\n",
    "   ts.end();\n",
    " } finally {\n",
    "   ts.close();\n",
    " }\n",
    " \n",
    " analyzer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40e1ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: term=今天是高兴的一天,bytes=[e4 bb 8a e5 a4 a9 e6 98 af e9 ab 98 e5 85 b4 e7 9a 84 e4 b8 80 e5 a4 a9],startOffset=0,endOffset=8,positionIncrement=1,positionLength=1,type=word,termFrequency=1\r\n"
     ]
    }
   ],
   "source": [
    "val analyzer = KeywordAnalyzer()\n",
    "val ts = analyzer.tokenStream(\"myfield\", StringReader(\"今天是高兴的一天\"));\n",
    "\n",
    " try {\n",
    "   ts.reset();\n",
    "   while (ts.incrementToken()) {\n",
    "     println(\"token: \" + ts.reflectAsString(false));\n",
    "   }\n",
    "   ts.end();\n",
    " } finally {\n",
    "   ts.close();\n",
    " }\n",
    " \n",
    " analyzer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983de7e",
   "metadata": {},
   "source": [
    "https://github.com/wks/ik-analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3b94c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: term=今天是,bytes=[e4 bb 8a e5 a4 a9 e6 98 af],startOffset=0,endOffset=3,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1\n",
      "token: term=今天,bytes=[e4 bb 8a e5 a4 a9],startOffset=0,endOffset=2,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1\n",
      "token: term=高兴,bytes=[e9 ab 98 e5 85 b4],startOffset=3,endOffset=5,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1\n",
      "token: term=一天,bytes=[e4 b8 80 e5 a4 a9],startOffset=6,endOffset=8,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1\n",
      "token: term=一,bytes=[e4 b8 80],startOffset=6,endOffset=7,positionIncrement=1,positionLength=1,type=TYPE_CNUM,termFrequency=1\n",
      "token: term=天,bytes=[e5 a4 a9],startOffset=7,endOffset=8,positionIncrement=1,positionLength=1,type=COUNT,termFrequency=1\n"
     ]
    }
   ],
   "source": [
    "val analyzer = IKAnalyzer();\n",
    "val ts = analyzer.tokenStream(\"myfield\", StringReader(\"今天是高兴的一天\"));\n",
    "\n",
    " try {\n",
    "   ts.reset();\n",
    "   while (ts.incrementToken()) {\n",
    "     println(\"token: \" + ts.reflectAsString(false));\n",
    "   }\n",
    "   ts.end();\n",
    " } finally {\n",
    "   ts.close();\n",
    " }\n",
    " \n",
    " analyzer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "codemirror_mode": "text/x-kotlin",
   "file_extension": ".kt",
   "mimetype": "text/x-kotlin",
   "name": "kotlin",
   "nbconvert_exporter": "",
   "pygments_lexer": "kotlin",
   "version": "1.8.0-dev-707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
